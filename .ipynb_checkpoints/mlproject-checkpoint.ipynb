{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6700871f-9c96-4d07-bbe4-8e8b274a3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d755216c-01b2-48f4-98fb-40b7e785b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/SAI VIGNESH CHINTALA/Desktop/ML1/breast-cancer.csv\"\n",
    "breast_cancer_data = pd.read_csv(file_path)\n",
    "\n",
    "# Create a new SQLite database (or connect to an existing one)\n",
    "conn = sqlite3.connect('C:/Users/SAI VIGNESH CHINTALA/Desktop/ML1/breast_cancer.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the SQL schema for Patient Information table\n",
    "create_patient_info_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS PatientInformation (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    diagnosis TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Define the SQL schema for Feature Statistics table\n",
    "create_feature_stats_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS FeatureStatistics (\n",
    "    id INTEGER,\n",
    "    radius_mean REAL,\n",
    "    texture_mean REAL,\n",
    "    perimeter_mean REAL,\n",
    "    area_mean REAL,\n",
    "    smoothness_mean REAL,\n",
    "    compactness_mean REAL,\n",
    "    concavity_mean REAL,\n",
    "    concave_points_mean REAL,\n",
    "    symmetry_mean REAL,\n",
    "    fractal_dimension_mean REAL,\n",
    "    radius_se REAL,\n",
    "    texture_se REAL,\n",
    "    perimeter_se REAL,\n",
    "    area_se REAL,\n",
    "    smoothness_se REAL,\n",
    "    compactness_se REAL,\n",
    "    concavity_se REAL,\n",
    "    concave_points_se REAL,\n",
    "    symmetry_se REAL,\n",
    "    fractal_dimension_se REAL,\n",
    "    radius_worst REAL,\n",
    "    texture_worst REAL,\n",
    "    perimeter_worst REAL,\n",
    "    area_worst REAL,\n",
    "    smoothness_worst REAL,\n",
    "    compactness_worst REAL,\n",
    "    concavity_worst REAL,\n",
    "    concave_points_worst REAL,\n",
    "    symmetry_worst REAL,\n",
    "    fractal_dimension_worst REAL,\n",
    "    FOREIGN KEY (id) REFERENCES PatientInformation(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the table creation queries\n",
    "cursor.execute(create_patient_info_table)\n",
    "cursor.execute(create_feature_stats_table)\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the tables from the breast cancer dataset\n",
    "patient_info_data = breast_cancer_data[['id', 'diagnosis']]\n",
    "feature_stats_data = breast_cancer_data.drop(columns=['diagnosis'])\n",
    "\n",
    "# Insert data into PatientInformation table\n",
    "patient_info_data.to_sql('PatientInformation', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Insert data into FeatureStatistics table\n",
    "feature_stats_data.to_sql('FeatureStatistics', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Add an index to improve query performance\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_patient_id ON FeatureStatistics(id);\")\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fcc63cc2-3187-4d63-92aa-b0721ec9b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  \n",
      "0          0.11840  \n",
      "1          0.08474  \n",
      "2          0.10960  \n",
      "3          0.14250  \n",
      "4          0.10030  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = 'C:/Users/SAI VIGNESH CHINTALA/Desktop/ML1/breast_cancer.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# SQL query to join PatientInformation and FeatureStatistics tables\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    PatientInformation.id, \n",
    "    PatientInformation.diagnosis, \n",
    "    FeatureStatistics.radius_mean, \n",
    "    FeatureStatistics.texture_mean, \n",
    "    FeatureStatistics.perimeter_mean, \n",
    "    FeatureStatistics.area_mean, \n",
    "    FeatureStatistics.smoothness_mean\n",
    "FROM \n",
    "    PatientInformation\n",
    "JOIN \n",
    "    FeatureStatistics\n",
    "ON \n",
    "    PatientInformation.id = FeatureStatistics.id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and load the result into a Pandas DataFrame\n",
    "joined_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(joined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0b27a877-4784-4774-bf13-5fd25815fc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Per Column:\n",
      "id                 0\n",
      "diagnosis          0\n",
      "radius_mean        0\n",
      "texture_mean       0\n",
      "perimeter_mean     0\n",
      "area_mean          0\n",
      "smoothness_mean    0\n",
      "dtype: int64\n",
      "Data Summary:\n",
      "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
      "count  4.380000e+02   438.000000    438.000000      438.000000   438.000000   \n",
      "mean   2.481861e+06    13.328801     18.714498       86.399018   569.692694   \n",
      "std    3.288493e+06     2.651980      3.951228       18.133148   233.678794   \n",
      "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
      "25%    8.645432e+05    11.512500     15.870000       73.885000   403.850000   \n",
      "50%    8.980115e+05    12.955000     18.325000       83.465000   515.200000   \n",
      "75%    9.222968e+05    14.867500     21.365000       96.442500   680.050000   \n",
      "max    9.113816e+06    19.790000     29.810000      132.400000  1192.000000   \n",
      "\n",
      "       smoothness_mean  \n",
      "count       438.000000  \n",
      "mean          0.095059  \n",
      "std           0.013103  \n",
      "min           0.062510  \n",
      "25%           0.085120  \n",
      "50%           0.094625  \n",
      "75%           0.103975  \n",
      "max           0.132300  \n",
      "Correlation Matrix:\n",
      "                       id  radius_mean  texture_mean  perimeter_mean  \\\n",
      "id               1.000000     0.080356      0.077484        0.077166   \n",
      "radius_mean      0.080356     1.000000      0.251722        0.997126   \n",
      "texture_mean     0.077484     0.251722      1.000000        0.258162   \n",
      "perimeter_mean   0.077166     0.997126      0.258162        1.000000   \n",
      "area_mean        0.076154     0.992648      0.263184        0.990693   \n",
      "smoothness_mean -0.054963     0.075392     -0.062251        0.115987   \n",
      "\n",
      "                 area_mean  smoothness_mean  \n",
      "id                0.076154        -0.054963  \n",
      "radius_mean       0.992648         0.075392  \n",
      "texture_mean      0.263184        -0.062251  \n",
      "perimeter_mean    0.990693         0.115987  \n",
      "area_mean         1.000000         0.079859  \n",
      "smoothness_mean   0.079859         1.000000  \n",
      "Cleaned data saved to C:/Users/SAI VIGNESH CHINTALA/Desktop/ML1/cleaned_breast_cancer_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Rename Columns\n",
    "joined_data.columns = joined_data.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Step 2: Remove Irrelevant Columns (if applicable)\n",
    "# Assuming no irrelevant columns in the current joined_data\n",
    "\n",
    "# Step 3: Handle Missing Data\n",
    "# Check for missing data\n",
    "missing_data = joined_data.isnull().sum()\n",
    "print(\"Missing Data Per Column:\")\n",
    "print(missing_data)\n",
    "\n",
    "# If there are missing values, drop rows with missing data\n",
    "joined_data = joined_data.dropna()\n",
    "\n",
    "# Step 4: Standardize Strings (if applicable)\n",
    "if 'diagnosis' in joined_data.columns:\n",
    "    joined_data['diagnosis'] = joined_data['diagnosis'].str.strip().str.upper()\n",
    "\n",
    "# Step 5: Correct Column Data Types\n",
    "# Ensure numeric columns are numeric\n",
    "for col in joined_data.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        joined_data[col] = pd.to_numeric(joined_data[col])\n",
    "    except ValueError:\n",
    "        pass  # Keep as object if conversion fails\n",
    "\n",
    "# Step 6: Identify and Remove Outliers\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "for col in joined_data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    joined_data = remove_outliers(joined_data, col)\n",
    "\n",
    "# Step 7: Explore Data\n",
    "print(\"Data Summary:\")\n",
    "print(joined_data.describe())\n",
    "\n",
    "# Calculate the Correlation Matrix (Exclude Non-Numeric Columns)\n",
    "numeric_columns = joined_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = joined_data[numeric_columns].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Step 8: Save the Cleaned Data\n",
    "cleaned_file_path = \"C:/Users/SAI VIGNESH CHINTALA/Desktop/ML1/cleaned_breast_cancer_data.csv\"\n",
    "joined_data.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8126863b-8031-4951-94ee-1407bfea10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4cf3d112-5dcd-4f37-b7b1-51c76fd14a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after loading: (438, 7)\n",
      "        id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0   842302          1        17.99         10.38          122.80     1001.0   \n",
      "5   843786          1        12.45         15.70           82.57      477.1   \n",
      "6   844359          1        18.25         19.98          119.60     1040.0   \n",
      "8   844981          1        13.00         21.82           87.50      519.8   \n",
      "10  845636          1        16.02         23.24          102.70      797.8   \n",
      "\n",
      "    smoothness_mean  \n",
      "0           0.11840  \n",
      "5           0.12780  \n",
      "6           0.09463  \n",
      "8           0.12730  \n",
      "10          0.08206  \n",
      "Selected features: ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean']\n",
      "Missing values:\n",
      " radius_mean        0\n",
      "texture_mean       0\n",
      "perimeter_mean     0\n",
      "area_mean          0\n",
      "smoothness_mean    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b37f5b58774960aae602f606864309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c34658edfbe47438d2682fef067b9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01fbb02069c4729a719fdce3e3f89c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c46f7662f34296954b4acab253ed7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y-Profile Report saved as 'breast_cancer_profile_report.html'\n",
      "Correlation matrix:\n",
      "                        id  diagnosis  radius_mean  texture_mean  \\\n",
      "id               1.000000  -0.024431     0.080356      0.077484   \n",
      "diagnosis       -0.024431   1.000000     0.700787      0.360001   \n",
      "radius_mean      0.080356   0.700787     1.000000      0.251722   \n",
      "texture_mean     0.077484   0.360001     0.251722      1.000000   \n",
      "perimeter_mean   0.077166   0.719516     0.997126      0.258162   \n",
      "area_mean        0.076154   0.720253     0.992648      0.263184   \n",
      "smoothness_mean -0.054963   0.312833     0.075392     -0.062251   \n",
      "\n",
      "                 perimeter_mean  area_mean  smoothness_mean  \n",
      "id                     0.077166   0.076154        -0.054963  \n",
      "diagnosis              0.719516   0.720253         0.312833  \n",
      "radius_mean            0.997126   0.992648         0.075392  \n",
      "texture_mean           0.258162   0.263184        -0.062251  \n",
      "perimeter_mean         1.000000   0.990693         0.115987  \n",
      "area_mean              0.990693   1.000000         0.079859  \n",
      "smoothness_mean        0.115987   0.079859         1.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Step 1: Map diagnosis to binary values\n",
    "if 'diagnosis' in joined_data.columns:\n",
    "    joined_data['diagnosis'] = joined_data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "else:\n",
    "    raise ValueError(\"Column 'diagnosis' is missing in the DataFrame.\")\n",
    "\n",
    "# Debug: Check DataFrame shape and head\n",
    "print(f\"DataFrame shape after loading: {joined_data.shape}\")\n",
    "print(joined_data.head())\n",
    "\n",
    "# Step 2: Handle missing or invalid data\n",
    "joined_data = joined_data.dropna(subset=['diagnosis'])  # Drop rows where diagnosis is NaN\n",
    "selected_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean']\n",
    "selected_features = [feature for feature in selected_features if feature in joined_data.columns]\n",
    "\n",
    "# Debug: Check selected features\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Fill missing values in selected features\n",
    "joined_data[selected_features] = joined_data[selected_features].fillna(joined_data[selected_features].median())\n",
    "\n",
    "# Debug: Check for missing values\n",
    "print(\"Missing values:\\n\", joined_data[selected_features].isnull().sum())\n",
    "\n",
    "# Step 3: Generate ydata-profiling report\n",
    "if not joined_data.empty:\n",
    "    profile = ProfileReport(joined_data, title=\"Breast Cancer Dataset Profiling Report\", explorative=True)\n",
    "    profile.to_file(\"breast_cancer_profile_report.html\")\n",
    "    print(\"Y-Profile Report saved as 'breast_cancer_profile_report.html'\")\n",
    "else:\n",
    "    print(\"DataFrame is empty. Skipping profiling.\")\n",
    "\n",
    "# Step 4: Correlation Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = joined_data.corr()\n",
    "\n",
    "# Debug: Check correlation matrix\n",
    "print(\"Correlation matrix:\\n\", correlation_matrix)\n",
    "\n",
    "if not correlation_matrix.empty:\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Correlation matrix is empty. Skipping heatmap.\")\n",
    "\n",
    "# Step 5: Boxplots for diagnosis vs features\n",
    "for feature in selected_features:\n",
    "    if feature in joined_data.columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.boxplot(\n",
    "            x=joined_data['diagnosis'], \n",
    "            y=joined_data[feature], \n",
    "            palette=\"Set2\"\n",
    "        )\n",
    "        plt.title(f\"Y-Profile for {feature} (Diagnosis vs {feature})\")\n",
    "        plt.xlabel(\"Diagnosis (0=Benign, 1=Malignant)\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Feature {feature} is not in the DataFrame. Skipping plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9bf426a3-b2ed-4389-86e8-d0b8c876f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116c5d1f117a42d5b9ae050414c5050a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e2e07266544814abf417fa54b7b342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7152d0e826400d8e6cf23e065204e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb170de91eb7478cb4754719dda9ae20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y-Profile Report saved as 'breast_cancer_profile_report.html'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Assuming `joined_data` is already loaded\n",
    "\n",
    "# Step 1: Map diagnosis to binary values\n",
    "if 'diagnosis' in joined_data.columns:\n",
    "    joined_data['diagnosis'] = joined_data['diagnosis'].map({'M': 1, 'B': 0})\n",
    "else:\n",
    "    raise ValueError(\"Column 'diagnosis' is missing in the DataFrame.\")\n",
    "\n",
    "# Step 2: Handle missing or invalid data\n",
    "joined_data = joined_data.dropna(subset=['diagnosis'])  # Drop rows where diagnosis is NaN\n",
    "selected_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean']\n",
    "selected_features = [feature for feature in selected_features if feature in joined_data.columns]\n",
    "\n",
    "# Fill missing values in selected features\n",
    "joined_data[selected_features] = joined_data[selected_features].fillna(joined_data[selected_features].median())\n",
    "\n",
    "# Step 3: Generate ydata-profiling report\n",
    "profile = ProfileReport(joined_data, title=\"Breast Cancer Dataset Profiling Report\", explorative=True)\n",
    "profile.to_file(\"breast_cancer_profile_report.html\")\n",
    "print(\"Y-Profile Report saved as 'breast_cancer_profile_report.html'\")\n",
    "\n",
    "# Step 4: Correlation Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = joined_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Boxplots for diagnosis vs features\n",
    "for feature in selected_features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(\n",
    "        x=joined_data['diagnosis'], \n",
    "        y=joined_data[feature], \n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    plt.title(f\"Y-Profile for {feature} (Diagnosis vs {feature})\")\n",
    "    plt.xlabel(\"Diagnosis (0=Benign, 1=Malignant)\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "99ac0318-ae26-427d-9955-9709ee3e32bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: (438, 7)\n",
      "        id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0   842302          1        17.99         10.38          122.80     1001.0   \n",
      "5   843786          1        12.45         15.70           82.57      477.1   \n",
      "6   844359          1        18.25         19.98          119.60     1040.0   \n",
      "8   844981          1        13.00         21.82           87.50      519.8   \n",
      "10  845636          1        16.02         23.24          102.70      797.8   \n",
      "\n",
      "    smoothness_mean  \n",
      "0           0.11840  \n",
      "5           0.12780  \n",
      "6           0.09463  \n",
      "8           0.12730  \n",
      "10          0.08206  \n"
     ]
    }
   ],
   "source": [
    "print(f\"DataFrame Shape: {joined_data.shape}\")\n",
    "print(joined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6c4d4a69-2345-4d64-9280-6ad0f26e7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape After Cleaning: (438, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"DataFrame Shape After Cleaning: {joined_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76188e1-23eb-4fa0-a095-619d46a5f422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, make_scorer, classification_report\n",
    "\n",
    "# Ensure MLFlow Tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'saivignesh-03'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '9c78979cb39e1c46900c7f95953a7fcb54a30dee'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Define preprocessing and model pipeline\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Numeric Transformer\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing numeric values\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('log', log_transformer)\n",
    "])\n",
    "\n",
    "# Categorical Transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categorical values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Logistic Regression Model\n",
    "logistic = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Combine preprocessing and model into a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', logistic)\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "kf_3fold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "kf_10fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classifier__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=make_scorer(f1_score), cv=kf_3fold, n_jobs=-1)\n",
    "\n",
    "# Log results in MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment #1: Logistic Regression with Preprocessing\"):\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Cross-validation results\n",
    "    f1_scores_3fold = cross_val_score(best_model, X, y, cv=kf_3fold, scoring=make_scorer(f1_score))\n",
    "    f1_scores_10fold = cross_val_score(best_model, X, y, cv=kf_10fold, scoring=make_scorer(f1_score))\n",
    "\n",
    "    y_pred = best_model.predict(X)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_param(\"preprocessing\", \"StandardScaler + MinMaxScaler + LogTransformation + OneHotEncoding\")\n",
    "    mlflow.log_metric(\"f1_mean_3fold\", np.mean(f1_scores_3fold))\n",
    "    mlflow.log_metric(\"f1_mean_10fold\", np.mean(f1_scores_10fold))\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "    # Log classification report\n",
    "    classification_report_str = classification_report(y, y_pred)\n",
    "    mlflow.log_text(classification_report_str, \"classification_report.txt\")\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(best_model, artifact_path=\"logistic_regression_model\", registered_model_name=\"LogisticRegression-Experiment-1\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExperiment #1 Completed:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"3-Fold F1 Score: Mean={np.mean(f1_scores_3fold):.4f}\")\n",
    "print(f\"10-Fold F1 Score: Mean={np.mean(f1_scores_10fold):.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24c7ab-1c1b-43e3-b842-26167f66de7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca4e10-c96b-4132-874a-5a720d18e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, make_scorer, classification_report\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'saivignesh-03'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '9c78979cb39e1c46900c7f95953a7fcb54a30dee'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Assuming X and y are already loaded and preprocessed\n",
    "# Ensure X (features) and y (target) are ready for model training\n",
    "\n",
    "# Preprocessing for numeric features\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('log', log_transformer)\n",
    "])\n",
    "\n",
    "# Combine preprocessors\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# List of classifiers to test\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"RidgeClassifier\": RidgeClassifier(random_state=42),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBClassifier\": XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "kf_10fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through classifiers and log results in MLFlow\n",
    "for clf_name, clf in classifiers.items():\n",
    "    with mlflow.start_run(run_name=f\"Experiment #2: {clf_name}\"):\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        f1_scores = cross_val_score(pipeline, X, y, cv=kf_10fold, scoring=make_scorer(f1_score))\n",
    "        acc_scores = cross_val_score(pipeline, X, y, cv=kf_10fold, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "        # Train the model on the full training data\n",
    "        pipeline.fit(X, y)\n",
    "        y_pred = pipeline.predict(X)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"classifier\", clf_name)\n",
    "        mlflow.log_param(\"preprocessing\", \"StandardScaler + MinMaxScaler + LogTransformation\")\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"f1_mean_10fold\", np.mean(f1_scores))\n",
    "        mlflow.log_metric(\"f1_std_10fold\", np.std(f1_scores))\n",
    "        mlflow.log_metric(\"accuracy_mean_10fold\", np.mean(acc_scores))\n",
    "        mlflow.log_metric(\"accuracy_std_10fold\", np.std(acc_scores))\n",
    "        mlflow.log_metric(\"f1_score_training\", f1_score(y, y_pred))\n",
    "        mlflow.log_metric(\"accuracy_training\", accuracy_score(y, y_pred))\n",
    "        mlflow.log_metric(\"true_positives\", tp)\n",
    "        mlflow.log_metric(\"true_negatives\", tn)\n",
    "        mlflow.log_metric(\"false_positives\", fp)\n",
    "        mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "        # Log model to MLflow\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path=f\"{clf_name}_model\", registered_model_name=f\"{clf_name}-Experiment-2\")\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nExperiment #2 Completed for {clf_name}:\")\n",
    "        print(f\"10-Fold F1 Score: Mean={np.mean(f1_scores):.4f}, Std={np.std(f1_scores):.4f}\")\n",
    "        print(f\"10-Fold Accuracy Score: Mean={np.mean(acc_scores):.4f}, Std={np.std(acc_scores):.4f}\")\n",
    "        print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f55499-ed01-4895-acb1-7c85ca235f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, make_scorer, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'saivignesh-03'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '9c78979cb39e1c46900c7f95953a7fcb54a30dee'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Assuming X and y are already loaded and preprocessed\n",
    "# Feature Engineering: Generate interaction terms and polynomial features\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Preprocessing pipeline\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('log', log_transformer),\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Feature selection: Remove low-variance features\n",
    "feature_selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "kf_10fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and log results in MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment #3: Feature Engineering with RandomForest\"):\n",
    "    # Perform 10-fold cross-validation\n",
    "    f1_scores = cross_val_score(pipeline, X, y, cv=kf_10fold, scoring=make_scorer(f1_score))\n",
    "    acc_scores = cross_val_score(pipeline, X, y, cv=kf_10fold, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "    # Train the model on the entire training data\n",
    "    pipeline.fit(X, y)\n",
    "    y_pred = pipeline.predict(X)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"preprocessing\", \"StandardScaler + MinMaxScaler + LogTransformation + PolynomialFeatures\")\n",
    "    mlflow.log_param(\"feature_selection\", \"VarianceThreshold\")\n",
    "    mlflow.log_param(\"classifier\", \"RandomForestClassifier\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"f1_mean_10fold\", np.mean(f1_scores))\n",
    "    mlflow.log_metric(\"f1_std_10fold\", np.std(f1_scores))\n",
    "    mlflow.log_metric(\"accuracy_mean_10fold\", np.mean(acc_scores))\n",
    "    mlflow.log_metric(\"accuracy_std_10fold\", np.std(acc_scores))\n",
    "    mlflow.log_metric(\"f1_score_training\", f1_score(y, y_pred))\n",
    "    mlflow.log_metric(\"accuracy_training\", accuracy_score(y, y_pred))\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "    # Log model to MLFlow\n",
    "    mlflow.sklearn.log_model(pipeline, artifact_path=\"feature_engineering_model\", registered_model_name=\"FeatureEngineering-Experiment-3\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nExperiment #3 Completed:\")\n",
    "    print(f\"10-Fold F1 Score: Mean={np.mean(f1_scores):.4f}, Std={np.std(f1_scores):.4f}\")\n",
    "    print(f\"10-Fold Accuracy Score: Mean={np.mean(acc_scores):.4f}, Std={np.std(acc_scores):.4f}\")\n",
    "    print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e5084-6327-4c54-b1b1-0aea2422f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'saivignesh-03'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '9c78979cb39e1c46900c7f95953a7fcb54a30dee'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('breast-cancer.csv')\n",
    "X = data.drop(columns=['id', 'diagnosis'])\n",
    "y = data['diagnosis'].map({'M': 1, 'B': 0})  # Convert diagnosis to binary\n",
    "\n",
    "# Validate dataset before processing\n",
    "if X.empty or y.empty:\n",
    "    raise ValueError(\"Dataset is empty. Please check your data.\")\n",
    "\n",
    "# Step 1: Remove constant and quasi-constant features\n",
    "variance_selector = VarianceThreshold(threshold=0.01)\n",
    "X_var = pd.DataFrame(\n",
    "    variance_selector.fit_transform(X),\n",
    "    columns=X.columns[variance_selector.get_support()],\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "# Step 2: Remove highly correlated features\n",
    "correlation_matrix = X_var.corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "X_uncorr = X_var.drop(columns=to_drop)\n",
    "\n",
    "# Step 3: Select features based on importance\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "rf_selector.fit(X_uncorr, y)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_uncorr.columns,\n",
    "    'importance': rf_selector.feature_importances_\n",
    "})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features based on cumulative importance\n",
    "cumulative_importance = importance_df['importance'].cumsum()\n",
    "n_features = (cumulative_importance <= 0.95).sum()\n",
    "top_features = importance_df['feature'].head(max(n_features, 10)).tolist()\n",
    "\n",
    "X_selected = X_uncorr[top_features]\n",
    "\n",
    "# Create final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation for selected features\n",
    "y_pred = cross_val_predict(pipeline, X_selected, y, cv=kf)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y, y_pred)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "# Log results with MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment 4: Optimized Feature Selection\"):\n",
    "    mlflow.log_param(\"original_features\", X.shape[1])\n",
    "    mlflow.log_param(\"selected_features\", X_selected.shape[1])\n",
    "    mlflow.log_param(\"variance_threshold\", 0.01)\n",
    "    mlflow.log_param(\"correlation_threshold\", 0.95)\n",
    "    mlflow.log_param(\"importance_threshold\", 0.95)\n",
    "    \n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "    \n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"feature_selection_model_optimized\",\n",
    "        registered_model_name=\"FeatureSelection-Optimized-Experiment-4\"\n",
    "    )\n",
    "\n",
    "# Print comprehensive results\n",
    "print(\"\\nExperiment #4 Completed:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"10-Fold F1 Score: Mean={f1:.4f}\")\n",
    "print(f\"10-Fold Accuracy Score: Mean={accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "\n",
    "# Classification Report\n",
    "classification_report_str = classification_report(y, y_pred, target_names=['0 (Benign)', '1 (Malignant)'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f410521-67b4-42e4-b034-a87ce20a744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'saivignesh-03'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '9c78979cb39e1c46900c7f95953a7fcb54a30dee'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('breast-cancer.csv')\n",
    "X = data.drop(columns=['id', 'diagnosis'])\n",
    "y = data['diagnosis'].map({'M': 1, 'B': 0})  # Convert diagnosis to binary\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('scree_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Select number of components to explain 95% variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"Number of components selected to explain 95% variance: {n_components}\")\n",
    "\n",
    "# PCA with selected components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca_selected = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Final Pipeline with PCA and Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Ensure standardization in the pipeline\n",
    "    ('pca', PCA(n_components=n_components)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "y_pred = cross_val_predict(pipeline, X, y, cv=kf)\n",
    "\n",
    "# Metrics\n",
    "f1 = f1_score(y, y_pred)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "# Log results in MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment 5: PCA Dimensionality Reduction\"):\n",
    "    mlflow.log_param(\"original_features\", X.shape[1])\n",
    "    mlflow.log_param(\"selected_components\", n_components)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "    # Save and log the scree plot\n",
    "    mlflow.log_artifact(\"scree_plot.png\")\n",
    "\n",
    "    # Log the PCA model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"pca_dimensionality_reduction_model\",\n",
    "        registered_model_name=\"PCA-DimensionalityReduction-Experiment-5\"\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExperiment #5 Completed:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Number of Components: {n_components}\")\n",
    "print(f\"10-Fold F1 Score: {f1:.4f}\")\n",
    "print(f\"10-Fold Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "\n",
    "# Classification Report\n",
    "classification_report_str = classification_report(y, y_pred, target_names=['0 (Benign)', '1 (Malignant)'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756b57b-0ac5-401a-8a61-811130e1631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"saivignesh-03\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"9c78979cb39e1c46900c7f95953a7fcb54a30dee\"\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"breast-cancer.csv\")\n",
    "X = data.drop(columns=[\"id\", \"diagnosis\"])\n",
    "y = data[\"diagnosis\"].map({\"M\": 1, \"B\": 0})  # Convert diagnosis to binary\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Base Models for Stacking\n",
    "base_models = [\n",
    "    (\"random_forest\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight=\"balanced\")),\n",
    "    (\"gradient_boosting\", GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "# Meta Model\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "# Pipeline with stacking\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"stacking\", stacking_clf),\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "y_pred = cross_val_predict(pipeline, X_scaled, y, cv=kf)\n",
    "\n",
    "# Metrics\n",
    "f1 = f1_score(y, y_pred)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "# Classification Report\n",
    "classification_report_str = classification_report(y, y_pred, target_names=[\"0 (Benign)\", \"1 (Malignant)\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.matshow(conf_matrix, cmap=\"coolwarm\", fignum=1)\n",
    "plt.title(\"Confusion Matrix\", pad=20)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"confusion_matrix_experiment_6.png\")\n",
    "plt.close()\n",
    "\n",
    "# Log results with MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment 6: Stacked Ensemble Model\"):\n",
    "    mlflow.log_param(\"base_models\", [model[0] for model in base_models])\n",
    "    mlflow.log_param(\"meta_model\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"random_forest_n_estimators\", 100)\n",
    "    mlflow.log_param(\"gradient_boosting_n_estimators\", 100)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "    # Log confusion matrix plot\n",
    "    mlflow.log_artifact(\"confusion_matrix_experiment_6.png\")\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"stacked_ensemble_model\",\n",
    "        registered_model_name=\"Stacked-Ensemble-Experiment-6\"\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExperiment #6 Completed:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"10-Fold F1 Score: {f1:.4f}\")\n",
    "print(f\"10-Fold Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edb20c-12ea-4d27-bebd-8ded9431511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up MLFlow tracking\n",
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/saivignesh-03/Machinelearning.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"saivignesh-03\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"9c78979cb39e1c46900c7f95953a7fcb54a30dee\"\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Machinelearning\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"breast-cancer.csv\")\n",
    "X = data.drop(columns=[\"id\", \"diagnosis\"])\n",
    "y = data[\"diagnosis\"].map({\"M\": 1, \"B\": 0})  # Convert diagnosis to binary\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define individual classifiers\n",
    "logistic = LogisticRegression(random_state=42, max_iter=1000)\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_classifier_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"logistic\", logistic),\n",
    "        (\"random_forest\", random_forest),\n",
    "        (\"knn\", knn)\n",
    "    ],\n",
    "    voting=\"soft\"  # Use soft voting for probabilities\n",
    ")\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "y_pred = cross_val_predict(voting_classifier_model, X_scaled, y, cv=kf, method=\"predict\")\n",
    "\n",
    "# Metrics\n",
    "f1 = f1_score(y, y_pred)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "# Classification Report\n",
    "classification_report_str = classification_report(y, y_pred, target_names=[\"0 (Benign)\", \"1 (Malignant)\"])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.matshow(conf_matrix, cmap=\"coolwarm\", fignum=1)\n",
    "plt.title(\"Confusion Matrix\", pad=20)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"confusion_matrix_experiment_7_voting.png\")\n",
    "plt.close()\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "voting_classifier_model.fit(X_scaled, y)\n",
    "\n",
    "# Log results with MLFlow\n",
    "with mlflow.start_run(run_name=\"Experiment 7: Voting Classifier\"):\n",
    "    mlflow.log_param(\"voting\", \"soft\")\n",
    "    mlflow.log_param(\"models_used\", [\"LogisticRegression\", \"RandomForestClassifier\", \"KNeighborsClassifier\"])\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"true_positives\", tp)\n",
    "    mlflow.log_metric(\"true_negatives\", tn)\n",
    "    mlflow.log_metric(\"false_positives\", fp)\n",
    "    mlflow.log_metric(\"false_negatives\", fn)\n",
    "    \n",
    "    # Log confusion matrix plot\n",
    "    mlflow.log_artifact(\"confusion_matrix_experiment_7_voting.png\")\n",
    "    \n",
    "    # Log the voting classifier\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=voting_classifier_model,\n",
    "        artifact_path=\"voting_classifier_model\",\n",
    "        registered_model_name=\"VotingClassifier-Experiment-7\"\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExperiment #7 Completed:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"10-Fold F1 Score: {f1:.4f}\")\n",
    "print(f\"10-Fold Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8155eb-afe4-41f6-a324-24f4f5965e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure inline plots in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set experiment name\n",
    "experiment_name = \"Machinelearning\"  # Replace with your experiment name\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if not experiment:\n",
    "    print(f\"Experiment '{experiment_name}' not found. Please check the name.\")\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "    # Retrieve all runs for the experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "    # Debug: Print all columns to check available metrics and parameters\n",
    "    print(\"Columns in the runs DataFrame:\")\n",
    "    print(runs.columns)\n",
    "\n",
    "    # Select metrics of interest based on available metrics in your MLflow\n",
    "    metrics_of_interest = ['f1_score', 'accuracy']  # Add or adjust as needed\n",
    "    if not all([f'metrics.{m}' in runs.columns for m in metrics_of_interest]):\n",
    "        print(f\"Some metrics in {metrics_of_interest} are not found in the runs.\")\n",
    "    else:\n",
    "        # Select and rename columns\n",
    "        comparison_df = runs[['run_id', 'tags.mlflow.runName'] + [f'metrics.{m}' for m in metrics_of_interest]]\n",
    "        comparison_df.rename(columns={\n",
    "            'tags.mlflow.runName': 'run_name',\n",
    "            'metrics.f1_score': 'f1_score',\n",
    "            'metrics.accuracy': 'accuracy'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Drop rows without required metrics\n",
    "        comparison_df.dropna(subset=['f1_score', 'accuracy'], inplace=True)\n",
    "\n",
    "        if comparison_df.empty:\n",
    "            print(\"No valid runs found with the required metrics.\")\n",
    "        else:\n",
    "            # Print the comparison DataFrame\n",
    "            print(\"Comparison of Experiments:\")\n",
    "            print(comparison_df[['run_name', 'f1_score', 'accuracy']])\n",
    "\n",
    "            # Plot F1-score Comparison\n",
    "            comparison_df.sort_values(by='f1_score', ascending=False, inplace=True)\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.barh(comparison_df['run_name'], comparison_df['f1_score'], color='skyblue')\n",
    "            plt.xlabel('F1-score', fontsize=12)\n",
    "            plt.ylabel('Experiments', fontsize=12)\n",
    "            plt.title('F1-score Comparison of Experiments', fontsize=14)\n",
    "            plt.gca().invert_yaxis()  # Show the best at the top\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('f1_score_comparison.png')\n",
    "            plt.show()\n",
    "\n",
    "            # Plot Accuracy Comparison\n",
    "            comparison_df.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.barh(comparison_df['run_name'], comparison_df['accuracy'], color='lightgreen')\n",
    "            plt.xlabel('Accuracy', fontsize=12)\n",
    "            plt.ylabel('Experiments', fontsize=12)\n",
    "            plt.title('Accuracy Comparison of Experiments', fontsize=14)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('accuracy_comparison.png')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b37c1-3869-4c78-90d0-c295a53a44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for comparison\n",
    "data = {\n",
    "    'run_name': [\n",
    "        \"Experiment 7: Voting Classifier\",\n",
    "        \"Experiment 6: Stacked Ensemble Model\",\n",
    "        \"Experiment 5: PCA Dimensionality Reduction\",\n",
    "        \"Experiment 4: Optimized Feature Selection\"\n",
    "    ],\n",
    "    'f1_score': [0.961353, 0.949640, 0.940898, 0.926366],\n",
    "    'accuracy': [0.971880, 0.963093, 0.956063, 0.945518]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Find the best model based on F1 score and Accuracy\n",
    "best_model = df.loc[df['f1_score'].idxmax()]\n",
    "print(\"Best Model Based on F1 Score:\")\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00bc98-dffd-4ec9-a312-1312ccfdf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model from MLFlow or use the existing instance\n",
    "# If `voting_classifier_model` is the already trained model instance:\n",
    "joblib.dump(voting_classifier_model, \"final_model.joblib\")\n",
    "\n",
    "print(\"Model saved as 'final_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54713ff-9e6c-4e30-9a69-29369036a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(voting_classifier_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab6402-7ae6-4256-868a-3f08f9678a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(\"final_model.joblib\")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b695b6-57d5-486c-87b5-b09840f636f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the trained Voting Classifier model\n",
    "voting_clf = joblib.load(\"final_model.joblib\")\n",
    "\n",
    "# Load the original scaler used during training\n",
    "scaler = joblib.load(\"scaler.joblib\")  # Ensure the scaler was saved during training\n",
    "\n",
    "# Example malignant sample data (ensure 30 features match the training features)\n",
    "sample_data = np.array([[\n",
    "    25.0, 30.0, 150.0, 1200.0, 0.160, 0.320, 0.450, 0.250, 0.350, 0.120,  # Mean features\n",
    "    0.300, 0.400, 0.500, 0.700, 0.080, 0.200, 0.300, 0.400, 0.600, 0.090,  # Standard error features\n",
    "    20.0, 35.0, 170.0, 1400.0, 0.190, 0.350, 0.480, 0.270, 0.400, 0.150   # Worst features\n",
    "]])  # Ensure the dimensions match the original feature set (30 features)\n",
    "\n",
    "# Scale the sample data using the loaded scaler\n",
    "sample_data_scaled = scaler.transform(sample_data)\n",
    "\n",
    "# Predict using the loaded Voting Classifier\n",
    "prediction = voting_clf.predict(sample_data_scaled)\n",
    "\n",
    "# Output prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"Prediction: Malignant (1)\")\n",
    "else:\n",
    "    print(\"Prediction: Benign (0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1bf4d-e38f-40ff-8d63-f182ce830da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, \"scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687cd4c-c35a-4981-bf00-1a98ab121d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the trained Voting Classifier model\n",
    "voting_clf = joblib.load(\"final_model.joblib\")\n",
    "\n",
    "# Load the original scaler used during training\n",
    "scaler = joblib.load(\"scaler.joblib\")  # Ensure the scaler was saved during training\n",
    "\n",
    "# Example malignant sample data (ensure 30 features match the training features)\n",
    "sample_data = np.array([[\n",
    "    25.0, 30.0, 150.0, 1200.0, 0.160, 0.320, 0.450, 0.250, 0.350, 0.120,  # Mean features\n",
    "    0.300, 0.400, 0.500, 0.700, 0.080, 0.200, 0.300, 0.400, 0.600, 0.090,  # Standard error features\n",
    "    20.0, 35.0, 170.0, 1400.0, 0.190, 0.350, 0.480, 0.270, 0.400, 0.150   # Worst features\n",
    "]])  # Ensure the dimensions match the original feature set (30 features)\n",
    "\n",
    "# Scale the sample data using the loaded scaler\n",
    "sample_data_scaled = scaler.transform(sample_data)\n",
    "\n",
    "# Predict using the loaded Voting Classifier\n",
    "prediction = voting_clf.predict(sample_data_scaled)\n",
    "\n",
    "# Output prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"Prediction: Malignant (1)\")\n",
    "else:\n",
    "    print(\"Prediction: Benign (0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11c1bd-6679-4329-8a99-d567b99cabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example malignant sample data with 30 features\n",
    "sample_data = np.array([[\n",
    "    25.0, 30.0, 150.0, 1200.0, 0.160, 0.320, 0.450, 0.250, 0.350, 0.120,  # Mean features\n",
    "    0.300, 0.400, 0.500, 0.700, 0.080, 0.200, 0.300, 0.400, 0.600, 0.090,  # Standard error features\n",
    "    20.0, 35.0, 170.0, 1400.0, 0.190, 0.350, 0.480, 0.270, 0.400, 0.150   # Worst features\n",
    "]])\n",
    "\n",
    "print(sample_data.shape)  # Ensure the dimensions are correct (1, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33995464-334c-40db-8bf9-bd83ec9ace60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the trained Voting Classifier model\n",
    "voting_clf = joblib.load(\"final_model.joblib\")\n",
    "\n",
    "# Load the original scaler used during training\n",
    "scaler = joblib.load(\"scaler.joblib\")  # Ensure the scaler was saved during training\n",
    "\n",
    "# Example malignant sample data (ensure 30 features match the training features)\n",
    "sample_data = np.array([[\n",
    "    28.0, 35.0, 180.0, 1500.0, 0.200, 0.400, 0.550, 0.300, 0.450, 0.150,  # Mean features\n",
    "    0.350, 0.450, 0.600, 0.800, 0.100, 0.250, 0.350, 0.500, 0.700, 0.120,  # Standard error features\n",
    "    25.0, 40.0, 200.0, 1800.0, 0.250, 0.450, 0.600, 0.400, 0.550, 0.200   # Worst features\n",
    "]])  # Ensure the dimensions match the original feature set (30 features)\n",
    "\n",
    "# Scale the sample data using the loaded scaler\n",
    "sample_data_scaled = scaler.transform(sample_data)\n",
    "\n",
    "# Predict using the loaded Voting Classifier\n",
    "prediction = voting_clf.predict(sample_data_scaled)\n",
    "\n",
    "# Output prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"Prediction: Malignant (1)\")\n",
    "else:\n",
    "    print(\"Prediction: Benign (0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993cc42d-8c6d-4983-9753-13b96eef9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn pydantic joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bf2d5-2ec7-4c61-9244-cf01ea4f33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the trained model and scaler\n",
    "try:\n",
    "    voting_clf = joblib.load(\"final_model.joblib\")\n",
    "    scaler = joblib.load(\"scaler.joblib\")\n",
    "except FileNotFoundError as e:\n",
    "    raise RuntimeError(f\"Required model or scaler file is missing: {e}\")\n",
    "\n",
    "# Define feature names\n",
    "feature_names = [\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n",
    "    \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n",
    "    \"compactness_se\", \"concavity_se\", \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n",
    "    \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Define the input schema\n",
    "class BreastCancerPredictionInput(BaseModel):\n",
    "    inputs: dict[str, float] = Field(\n",
    "        ...,\n",
    "        description=\"Dictionary of feature names and their corresponding values.\",\n",
    "        example={name: 0.0 for name in feature_names},\n",
    "    )\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the Breast Cancer Prediction API!\"}\n",
    "\n",
    "# Endpoint to retrieve feature names\n",
    "@app.get(\"/features/\")\n",
    "async def get_features():\n",
    "    \"\"\"Returns the feature names expected for the prediction.\"\"\"\n",
    "    return {\"feature_names\": feature_names}\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(input_data: BreastCancerPredictionInput):\n",
    "    input_features = input_data.inputs\n",
    "\n",
    "    # Validate missing features\n",
    "    missing_features = [feature for feature in feature_names if feature not in input_features]\n",
    "    if missing_features:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Missing features: {missing_features}\",\n",
    "        )\n",
    "\n",
    "    # Validate extra features\n",
    "    extra_features = [feature for feature in input_features if feature not in feature_names]\n",
    "    if extra_features:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Unexpected features: {extra_features}\",\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Extract feature values in the correct order\n",
    "        feature_values = [input_features[feature] for feature in feature_names]\n",
    "        sample_data = np.array([feature_values])\n",
    "\n",
    "        # Scale the input\n",
    "        sample_data_scaled = scaler.transform(sample_data)\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = voting_clf.predict(sample_data_scaled)\n",
    "\n",
    "        return {\"prediction\": \"Malignant (1)\" if prediction[0] == 1 else \"Benign (0)\"}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Prediction failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    import nest_asyncio  # Required for running in Jupyter\n",
    "\n",
    "    # Apply nest_asyncio to allow uvicorn to run in Jupyter's event loop\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Start the FastAPI server\n",
    "    print(\"Starting FastAPI server... Access the API docs at http://127.0.0.1:8000/docs\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be869f36-3d32-4005-9ed6-6714684e62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029f9af-da74-481a-871e-2add0d1b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c39e2d-ddbc-41e7-8362-ee36e9225326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the trained model and scaler\n",
    "try:\n",
    "    voting_clf = joblib.load(\"final_model.joblib\")\n",
    "    scaler = joblib.load(\"scaler.joblib\")\n",
    "except FileNotFoundError as e:\n",
    "    raise RuntimeError(f\"Required model or scaler file is missing: {e}\")\n",
    "\n",
    "# Define feature names\n",
    "feature_names = [\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\",\n",
    "    \"compactness_mean\", \"concavity_mean\", \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\",\n",
    "    \"compactness_se\", \"concavity_se\", \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\",\n",
    "    \"compactness_worst\", \"concavity_worst\", \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Define default values\n",
    "default_values = {\n",
    "    \"radius_mean\": 28.0,\n",
    "    \"texture_mean\": 35.0,\n",
    "    \"perimeter_mean\": 180.0,\n",
    "    \"area_mean\": 1500.0,\n",
    "    \"smoothness_mean\": 0.200,\n",
    "    \"compactness_mean\": 0.400,\n",
    "    \"concavity_mean\": 0.550,\n",
    "    \"concave_points_mean\": 0.300,\n",
    "    \"symmetry_mean\": 0.450,\n",
    "    \"fractal_dimension_mean\": 0.150,\n",
    "    \"radius_se\": 0.350,\n",
    "    \"texture_se\": 0.450,\n",
    "    \"perimeter_se\": 0.600,\n",
    "    \"area_se\": 0.800,\n",
    "    \"smoothness_se\": 0.100,\n",
    "    \"compactness_se\": 0.250,\n",
    "    \"concavity_se\": 0.350,\n",
    "    \"concave_points_se\": 0.500,\n",
    "    \"symmetry_se\": 0.700,\n",
    "    \"fractal_dimension_se\": 0.120,\n",
    "    \"radius_worst\": 25.0,\n",
    "    \"texture_worst\": 40.0,\n",
    "    \"perimeter_worst\": 200.0,\n",
    "    \"area_worst\": 1800.0,\n",
    "    \"smoothness_worst\": 0.250,\n",
    "    \"compactness_worst\": 0.450,\n",
    "    \"concavity_worst\": 0.600,\n",
    "    \"concave_points_worst\": 0.400,\n",
    "    \"symmetry_worst\": 0.550,\n",
    "    \"fractal_dimension_worst\": 0.200\n",
    "}\n",
    "\n",
    "# Define the input schema\n",
    "class BreastCancerPredictionInput(BaseModel):\n",
    "    inputs: dict[str, float] = Field(\n",
    "        ...,\n",
    "        description=\"Dictionary of feature names and their corresponding values.\",\n",
    "        example=default_values,  # Use default values here\n",
    "    )\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the Breast Cancer Prediction API!\"}\n",
    "\n",
    "# Endpoint to retrieve feature names\n",
    "@app.get(\"/features/\")\n",
    "async def get_features():\n",
    "    \"\"\"Returns the feature names expected for the prediction.\"\"\"\n",
    "    return {\"feature_names\": feature_names}\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(input_data: BreastCancerPredictionInput):\n",
    "    input_features = input_data.inputs\n",
    "\n",
    "    # Validate missing features\n",
    "    missing_features = [feature for feature in feature_names if feature not in input_features]\n",
    "    if missing_features:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Missing features: {missing_features}\",\n",
    "        )\n",
    "\n",
    "    # Validate extra features\n",
    "    extra_features = [feature for feature in input_features if feature not in feature_names]\n",
    "    if extra_features:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Unexpected features: {extra_features}\",\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Extract feature values in the correct order\n",
    "        feature_values = [input_features[feature] for feature in feature_names]\n",
    "        sample_data = np.array([feature_values])\n",
    "\n",
    "        # Scale the input\n",
    "        sample_data_scaled = scaler.transform(sample_data)\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = voting_clf.predict(sample_data_scaled)\n",
    "\n",
    "        return {\"prediction\": \"Malignant (1)\" if prediction[0] == 1 else \"Benign (0)\"}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Prediction failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    import nest_asyncio  # Required for running in Jupyter\n",
    "\n",
    "    # Apply nest_asyncio to allow uvicorn to run in Jupyter's event loop\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Start the FastAPI server\n",
    "    print(\"Starting FastAPI server... Access the API docs at http://127.0.0.1:8000/docs\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530d568-dd5a-4a85-a49e-34c57d1d18a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
